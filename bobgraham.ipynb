{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script attempts to predict a Bob Graham round finishing time based on the race results of other finishers. The membership list of the BGR is imported into the DUV ultramarathon databased to obtain runner ID's. The each race result is then scraped from the DUV site and the results of any of the Bob Graham finishers are then stored. \n",
    "\n",
    "The idea is then to input a specific race result that you've completed or specific distance stats to see where you compare to other finishers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        gn        fn gender  age   time        date  weekNum dir  mem  orig  \\\n",
      "0     Alan    Heaton      M   32  22:18  1960-06-26       25   A    1     0   \n",
      "1  Stanley  Bradshaw      M   48  23:25  1960-07-09       27   C    2     0   \n",
      "2  Kenneth    Heaton      M   35  22:13  1961-06-24       25   C    3     0   \n",
      "3     Eric     Beard      M   31  23:35  1963-07-06       27   C    4     0   \n",
      "4     Joss    Naylor      M   34  23:37  1971-06-26       25   C    5     0   \n",
      "\n",
      "   ...  opt  record  lr  died  nationality                       club prev  \\\n",
      "0  ...    0       1   0  2019      British  Clayton-le-Moors Harriers    0   \n",
      "1  ...    0       0   0  2010      British  Clayton-le-Moors Harriers    1   \n",
      "2  ...    0       1   0  2014      British  Clayton-le-Moors Harriers    0   \n",
      "3  ...    0       0   0  1969      British        Leeds Athletic Club    0   \n",
      "4  ...    1       0   0     0      British       Kendal Athletic Club    0   \n",
      "\n",
      "   postcode                                              notes reg_gap  \n",
      "0        S8  (R) Brother of Kenneth Heaton (3). * Record time.      25  \n",
      "1      BB12                                                NaN      27  \n",
      "2      BB12  (R) Brother of Alan Heaton (1). * Started at t...      25  \n",
      "3       LS6                                                NaN      26  \n",
      "4      CA20                                                NaN      25  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL pointing to the CSV data\n",
    "CSV_URL = \"http://bobgrahamclub.org.uk/api/data_bgr_listing.csv\"\n",
    "\n",
    "bg_results = pd.read_csv(CSV_URL)\n",
    "print(bg_results.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1:\n",
      "  Unnamed: 0_level_0 DUV Ultra Marathon Statistics                       \\\n",
      "             Counter                       Surname First name Runner ID   \n",
      "0                  1                        Heaton       Alan   1462093   \n",
      "1                  2                      Bradshaw    Stanley    Search   \n",
      "2                  3                        Heaton    Kenneth    Search   \n",
      "3                  4                         Beard       Eric   1048701   \n",
      "4                  5                        Naylor       Joss    Search   \n",
      "\n",
      "                                                                \n",
      "  Original name Nat.  M/F     YOB Date of birth Cat. internat.  \n",
      "0           NaN  GBR    M     NaN    00.00.0000            NaN  \n",
      "1           NaN  NaN  NaN     NaN           NaN            NaN  \n",
      "2           NaN  NaN  NaN     NaN           NaN            NaN  \n",
      "3           NaN  GBR    M  1931.0    00.00.0000            NaN  \n",
      "4           NaN  NaN  NaN     NaN           NaN            NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load DUV bulk search\n",
    "DUV_PATH = \"./data/DUV_Bulkexport_20240416.xls\"\n",
    "\n",
    "def load_html_to_dataframe(file_path):\n",
    "    try:\n",
    "        dfs = pd.read_html(file_path, encoding='utf-8')\n",
    "        return dfs\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the HTML file: {e}\")\n",
    "        return None\n",
    "\n",
    "data_frames = load_html_to_dataframe(DUV_PATH)\n",
    "if data_frames:\n",
    "    for i, df in enumerate(data_frames):\n",
    "        print(f\"Table {i+1}:\")\n",
    "        print(df.head())  # Display the first few rows of each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Counter</th>\n",
       "      <th>Surname</th>\n",
       "      <th>First name</th>\n",
       "      <th>Runner ID</th>\n",
       "      <th>Original name</th>\n",
       "      <th>Nat.</th>\n",
       "      <th>M/F</th>\n",
       "      <th>YOB</th>\n",
       "      <th>Date of birth</th>\n",
       "      <th>Cat. internat.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Heaton</td>\n",
       "      <td>Alan</td>\n",
       "      <td>1462093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GBR</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00.00.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Beard</td>\n",
       "      <td>Eric</td>\n",
       "      <td>1048701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GBR</td>\n",
       "      <td>M</td>\n",
       "      <td>1931.0</td>\n",
       "      <td>00.00.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Talbot</td>\n",
       "      <td>Donald</td>\n",
       "      <td>482320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GBR</td>\n",
       "      <td>M</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>00.00.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Weir</td>\n",
       "      <td>Dennis</td>\n",
       "      <td>113489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GBR</td>\n",
       "      <td>M</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>25.03.1938</td>\n",
       "      <td>M85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Millen</td>\n",
       "      <td>Boyd</td>\n",
       "      <td>1051631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GBR</td>\n",
       "      <td>M</td>\n",
       "      <td>1936.0</td>\n",
       "      <td>25.01.1936</td>\n",
       "      <td>M85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Counter Surname First name Runner ID  Original name Nat. M/F     YOB  \\\n",
       "0         1  Heaton       Alan   1462093            NaN  GBR   M     NaN   \n",
       "3         4   Beard       Eric   1048701            NaN  GBR   M  1931.0   \n",
       "6         7  Talbot     Donald    482320            NaN  GBR   M  1932.0   \n",
       "12       13    Weir     Dennis    113489            NaN  GBR   M  1938.0   \n",
       "13       14  Millen       Boyd   1051631            NaN  GBR   M  1936.0   \n",
       "\n",
       "   Date of birth Cat. internat.  \n",
       "0     00.00.0000            NaN  \n",
       "3     00.00.0000            NaN  \n",
       "6     00.00.0000            NaN  \n",
       "12    25.03.1938            M85  \n",
       "13    25.01.1936            M85  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_runners = df[df[('DUV Ultra Marathon Statistics', 'Runner ID')] != 'Search']\n",
    "valid_runners.columns = valid_runners.columns.droplevel(0)\n",
    "valid_runners.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def fetch_event_data(url):\n",
    "    \"\"\" Fetch and parse individual event data from a table with a specific ID. \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Use pandas to read the specific table by its ID\n",
    "            df_list = pd.read_html(response.content, attrs={'id': 'Resultlist'})\n",
    "            return df_list[0] if df_list else None\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url}: HTTP {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching event data from {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def fetch_html(url):\n",
    "    \"\"\" Fetch the HTML content of a given URL. \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    else:\n",
    "        print(f\"Failed to fetch {url}: HTTP {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def extract_page_count(soup):\n",
    "    \"\"\" Extract the total number of pages from pagination. \"\"\"\n",
    "    pagination = soup.find('div', class_='pagination')\n",
    "    if pagination:\n",
    "        last_link = pagination.find_all('a')[-2]  # assuming the second last link is the last page\n",
    "        last_page_number = int(last_link.get_text())\n",
    "        return last_page_number\n",
    "    return 1\n",
    "\n",
    "def extract_event_links(base_url, soup):\n",
    "    \"\"\" Extract event links from the page. \"\"\"\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if 'getresultevent.php?event=' in href:\n",
    "            links.append(base_url + href)\n",
    "    return links\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://statistik.d-u-v.org/\"\n",
    "    start_year = 2024\n",
    "    end_year = 2024  # Adjust as needed\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        page_url = f\"{base_url}geteventlist.php?year={year}&dist=all&country=all&surface=all&sort=1&page=1\"\n",
    "        first_page = fetch_html(page_url)\n",
    "        if first_page:\n",
    "            num_pages = extract_page_count(first_page)\n",
    "            all_data = []\n",
    "\n",
    "            for page in range(1, num_pages + 1):\n",
    "                page_url = f\"{base_url}geteventlist.php?year={year}&dist=all&country=all&surface=all&sort=1&page={page}\"\n",
    "                page_soup = fetch_html(page_url)\n",
    "                if page_soup:\n",
    "                    event_urls = extract_event_links(base_url, page_soup)\n",
    "                    for url in event_urls:\n",
    "                        event_data = fetch_event_data(url)\n",
    "                        if event_data is not None:\n",
    "                            all_data.append(event_data)\n",
    "\n",
    "            if all_data:\n",
    "                final_df = pd.concat(all_data, ignore_index=True)\n",
    "                final_df.to_csv(f'all_events_data_{year}.csv', index=False)\n",
    "                print(f\"Saved all event data for {year} to 'all_events_data_{year}.csv'.\")\n",
    "            else:\n",
    "                print(f\"No data was extracted for {year}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html(url):\n",
    "    \"\"\"Fetch the content of a URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    return BeautifulSoup(response.content, 'html.parser') if response.ok else None\n",
    "\n",
    "def extract_page_count(soup):\n",
    "    \"\"\"Extract the number of pages from the pagination element.\"\"\"\n",
    "    pagination = soup.find('div', class_='pagination')\n",
    "    return int(pagination.find_all('a')[-2].text) if pagination else 1\n",
    "\n",
    "def extract_event_links(base_url, soup):\n",
    "    \"\"\" Extract event links from the page. \"\"\"\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if 'getresultevent.php?event=' in href:\n",
    "            links.append(base_url + href)\n",
    "    return links\n",
    "\n",
    "def extract_event_details(soup):\n",
    "    \"\"\"Extract 'Event', 'Date', and 'Distance' from the HTML content.\"\"\"\n",
    "    details = {}\n",
    "    try:\n",
    "        info_rows = soup.find_all('tr')  # Find all table rows in the page\n",
    "        for row in info_rows:\n",
    "            # Look for rows where the first cell contains the labels we're interested in\n",
    "            header_cell = row.find('td')\n",
    "            if header_cell and header_cell.find('b'):  # Check for bold tags which might contain labels\n",
    "                label = header_cell.get_text(strip=True).rstrip(':')\n",
    "                value_cell = header_cell.find_next_sibling('td')  # Get the next sibling cell for the value\n",
    "                if label in ['Date', 'Event', 'Distance'] and value_cell:\n",
    "                    details[label] = value_cell.get_text(strip=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting event details: {e}\")\n",
    "    return details\n",
    "\n",
    "\n",
    "def fetch_event_data(table_soup, event_details):\n",
    "    \"\"\"Extract data and runner IDs from the event table, including event details.\"\"\"\n",
    "    data = []\n",
    "    headers = [th.text.strip() for th in table_soup.find_all('th')]\n",
    "    # Append the event details headers\n",
    "    headers.extend(['Runner ID', 'Event', 'Date', 'Distance'])\n",
    "\n",
    "    rows = table_soup.find_all('tr')[1:]  # Skip the header row\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        row_data = [col.text.strip() for col in cols]\n",
    "        # Get runner ID\n",
    "        link = cols[2].find('a', href=True)  # Assuming the third column has the link\n",
    "        runner_id = link['href'].split('runner=')[-1] if link else 'No ID'\n",
    "        # Include event details and runner ID\n",
    "        row_data.extend([runner_id, event_details['Event'], event_details['Date'], event_details['Distance']])\n",
    "        data.append(row_data)\n",
    "\n",
    "    return pd.DataFrame(data, columns=headers)\n",
    "\n",
    "def fetch_event_data(table_soup, event_details, valid_ids):\n",
    "    \"\"\"Extract data and runner IDs from the event table, appending event details, filtered by valid runner IDs.\"\"\"\n",
    "    data = []\n",
    "    headers = [th.text.strip() for th in table_soup.find_all('th')]\n",
    "    headers.extend(['Runner ID', 'Event', 'Date', 'Distance'])\n",
    "    rows = table_soup.find_all('tr')[1:]  # Skip header row\n",
    "    \n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) > 2:\n",
    "            link = cols[2].find('a', href=True)\n",
    "            if link and 'runner=' in link['href']:\n",
    "                runner_id = link['href'].split('runner=')[-1]\n",
    "                # Only add data if runner_id is in the list of valid_ids\n",
    "                if runner_id in valid_ids:\n",
    "                    row_data = [col.text.strip() for col in cols]\n",
    "                    row_data.extend([runner_id, event_details.get('Event', 'N/A'), event_details.get('Date', 'N/A'), event_details.get('Distance', 'N/A')])\n",
    "                    data.append(row_data)\n",
    "    \n",
    "    return pd.DataFrame(data, columns=headers)\n",
    "\n",
    "def scrape_events(base_url, event_url):\n",
    "    \"\"\"Scrape events with additional details.\"\"\"\n",
    "    soup = fetch_html(base_url + event_url)\n",
    "    event_details = extract_event_details(soup)\n",
    "    table_soup = soup.find('table', {'id': 'Resultlist'})\n",
    "    if table_soup:\n",
    "        return fetch_event_data(table_soup, event_details)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "# URL configuration\n",
    "base_url = \"https://statistik.d-u-v.org/\"\n",
    "event_url = \"getresultevent.php?event=102988\"\n",
    "\n",
    "# Perform the scraping\n",
    "#event_data = scrape_events(base_url, event_url)\n",
    "\n",
    "# Display or export the data\n",
    "#ßprint(event_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    base_url = \"https://statistik.d-u-v.org/\"\n",
    "    start_year = 2020\n",
    "    end_year = 2024  # Adjust as needed\n",
    "    valid_ids = set(valid_runners[\"Runner ID\"])\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        page_url = f\"{base_url}geteventlist.php?year={year}&dist=all&country=all&surface=all&sort=1&page=1\"\n",
    "        first_page = fetch_html(page_url)\n",
    "        if first_page:\n",
    "            num_pages = extract_page_count(first_page)\n",
    "            all_data = []\n",
    "\n",
    "            for page in range(1, num_pages + 1):\n",
    "                page_url = f\"{base_url}geteventlist.php?year={year}&dist=all&country=all&surface=all&sort=1&page={page}\"\n",
    "                page_soup = fetch_html(page_url)\n",
    "                if page_soup:\n",
    "                    event_links = extract_event_links(base_url, page_soup)\n",
    "                    for event_link in event_links:\n",
    "                        event_page = fetch_html(event_link)\n",
    "                        if event_page:\n",
    "                            event_details = extract_event_details(event_page)\n",
    "                            table_soup = event_page.find('table', {'id': 'Resultlist'})\n",
    "                            if table_soup:\n",
    "                                event_data = fetch_event_data(table_soup, event_details, valid_ids)\n",
    "                                if not event_data.empty:\n",
    "                                    all_data.append(event_data)\n",
    "\n",
    "            if all_data:\n",
    "                final_df = pd.concat(all_data, ignore_index=True)\n",
    "                final_df.to_csv(f'all_events_data_{year}.csv', index=False)\n",
    "                print(f\"Saved all event data for {year} to 'all_events_data_{year}.csv'.\")\n",
    "            else:\n",
    "                print(f\"No data was extracted for {year}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1462093\n",
       "3       1048701\n",
       "6        482320\n",
       "12       113489\n",
       "13      1051631\n",
       "         ...   \n",
       "2858    1361288\n",
       "2860    1168335\n",
       "2861     653340\n",
       "2862     861254\n",
       "2863     995441\n",
       "Name: Runner ID, Length: 1092, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
